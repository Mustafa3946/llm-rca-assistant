# =============================================================================
# rag_engine.py
#
# Purpose:
#   This module implements the core Retrieval-Augmented Generation (RAG) pipeline for the RCA assistant.
#   It retrieves the most relevant log messages for a user query, constructs a prompt for the LLM,
#   and returns a concise root cause analysis generated by the language model.
# =============================================================================

from src.retriever import get_top_k_logs
from src.mock_bedrock import query_llm

def generate_prompt(query: str, retrieved_logs: list) -> str:
    # Constructs a prompt for the LLM using the user query and the retrieved log messages.
    logs_text = "\n\n".join([f"- {log}" for _, log in retrieved_logs])
    return (
        f"You are an RCA assistant. Given the following system logs and the user query, "
        f"identify the most likely root cause in a concise and technical explanation.\n\n"
        f"User Query:\n{query}\n\nRelevant Logs:\n{logs_text}"
    )

def rca_pipeline(query: str, k: int = 3) -> list[str]:
    # Main RAG pipeline:
    # 1. Retrieve the top-k most relevant logs for the query.
    # 2. Generate a prompt combining the query and logs.
    # 3. Query the LLM with the prompt and return the response as a list.
    retrieved_logs = get_top_k_logs(query, k=k)
    prompt = generate_prompt(query, retrieved_logs)
    response = query_llm(prompt)
    return [response]